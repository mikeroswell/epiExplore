---
title: "GiniKappaK"
author: "Michael Roswell"
date: "2024-04-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Overview 

$a$, the shape parameter of a gamma, or size/dispresion parameter of a negative
binomial, is the canonical but confusing way of describing nb dispersion. It is
confusing b/c it is inversely related to dispersion. It can be re-written more
generally (see below), allowing us to see that $a$ shows pathological behavior
as $\mu \to \sigma$, approaching positive infinity when $\sigma > \mu$ and
positive infinity when $\mu > \sigma$. While $a$ is a canonical parameter for
gamma & negative-binomial distributions, it may be less intuitive for other
families of distributions.

Several other measures of dispersion may be computed either based on the moments
of a distribution or a sample from it, for example, the coefficient of variation
$CV = \sigma / \mu$, its cousin, $\kappa = CV^2 = \sigma^2/\mu^2$, and the
Gini coefficient,
$G = \frac{\sum_{i}\sum_{j} |x_i - x_j|}{2\sum_{i}\sum_{j}x_i}$. 

# Properties and relationships among measures and distributions

## the gamma distribution
The mean of a gamma distribution with shape $a$ and rate $\theta$ is the shape
times the rate, or $a* \theta$, and the variance is $k\theta^2$, which means
that $sigma^2/\mu^2 = \kappa = 1/a$. Furthermore, $\mu/\sigma^2 = \theta$ and
the coefficient of variation, $\sigma/\mu = \sqrt{\kappa}= 1/\sqrt{a}$

## the negative binomial distribution 
The negative binomial can be considered a Poisson sampling of a
gamma-distributed mean. If the size parameter is $a$ and the probability $p$,
$nb(a, p) = Pois(gamma(a, p/(1-p))$ The mean of this distribution is $a(1-p)/p$
and the variance is $a(1-p)/p^2$, so re-writing $a$ in terms of the first two
moments of this distribution we get $\frac{\mu^2}{\sigma^2 - \mu}$, sublty
different from $1/\kappa$. When $\mu = \sigma^2$, $a$ is infinite, and the sign
depends on whether you approach from variance > mean (positive) or variance <
mean (negative). By contrast, when the mean = variance, $\kappa = 1/mu$, when the
variance is less than the mean, $\kappa$ can get very small, but there is no
singularity until variance approaches 0; $\kappa \to \inf$ and $a \to 0$ as the
variance gets big w.r.t. the mean. For $\kappa = 1$, $\mu = \sigma$, a cool
point to pay attention to as well (this is also a property of the CV); when 
$a =1$, $\mu^2 = \sigma^2-\mu$, which is not a super intuitive point value.

## The Poisson distribution
As $a \to \inf$, $\kappa \to \mu$, as $n, \mu$ get large, the Gini coefficient
approaches 0 (no inequality). $1- \mu/sigma^2$ is more Gini-esque, bounded by 1
at super high inequality (a negative binomial with a high mean and a very low
size/k parameter) and 0 (a Poisson with a high mean), though they scale
differently with $k$.

## Bounds + signs
$a$ approaches infinity from opposite sides when the variance is greater vs.
smaller than the mean. In the gamma/ negative binomial context, it is strictly
positive; in the more generalized moment-based definition it could be negative
any time $\mu > \sigma^2$.

In the gamma/NB context, $\kappa$  is also strictly positive and $\geq \mu$, but
it can be smaller in an underdispersed distribution, or (not pertinent to our
disease stuff, I think) negative, in any distribution with a negative mean.

Both $a$ decreases with dispersion/inequality. The Gini Coefficient is bounded
between 0 and 1 for any distribution. It, like $\kappa$, increases with
inequality/dispersion

The CV grows with dispersion, and cv=1 



